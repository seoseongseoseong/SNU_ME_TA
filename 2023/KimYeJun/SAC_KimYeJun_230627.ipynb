{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNtCY9vg-5nO"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ThVHhBC75ddk",
        "outputId": "f897f9a2-67c9-4fdd-950b-ba64df219342"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TDUq_-Mg5gDe",
        "outputId": "b14f6d4b-56a1-4841-b9bc-455636eb447f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/baseline0513\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Colab Notebooks/baseline0513"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a8M8nn-35htN",
        "outputId": "c37e176c-a43a-4869-d1a0-36e069243f70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting fmpy\n",
            "  Downloading FMPy-0.3.15-py3-none-any.whl (10.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m45.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from fmpy) (23.1.0)\n",
            "Requirement already satisfied: Jinja2 in /usr/local/lib/python3.10/dist-packages (from fmpy) (3.1.2)\n",
            "Collecting lark (from fmpy)\n",
            "  Downloading lark-1.1.5-py3-none-any.whl (107 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.0/108.0 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from fmpy) (4.9.2)\n",
            "Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from fmpy) (1.0.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fmpy) (1.22.4)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from fmpy) (2022.7.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2->fmpy) (2.1.3)\n",
            "Installing collected packages: lark, fmpy\n",
            "Successfully installed fmpy-0.3.15 lark-1.1.5\n"
          ]
        }
      ],
      "source": [
        "!pip install fmpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5NTl-iWo5lll"
      },
      "outputs": [],
      "source": [
        "from fmpy import read_model_description, extract\n",
        "from fmpy.fmi2 import FMU2Slave\n",
        "from fmpy.util import plot_result, download_test_file\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import shutil\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "import torch\n",
        "import argparse\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "from tqdm import tqdm\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import scipy.signal\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "jzy0fVia5nOt"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions.normal import Normal\n",
        "from torch import FloatTensor as FT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FFb--wHb5pMG"
      },
      "outputs": [],
      "source": [
        "import tensorflow_probability as tfp\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pgyKpwfUB04W"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXZrJSRD-8b1"
      },
      "source": [
        "# ENV"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "84MjgSIK5qlu"
      },
      "outputs": [],
      "source": [
        "fmu_filename = 'HEV_TMED_Simulator_Rearrange_230511_linux.fmu'\n",
        "start_time = 0.0\n",
        "stop_time = 1800.0\n",
        "step_size = 0.01\n",
        "soc_init = 67\n",
        "\n",
        "# WLTP profile에 대하여 시험\n",
        "with open(\"wltp_vehicle_speed_profile_real.csv\") as file_name:\n",
        "    vehicle_speed_profile = np.loadtxt(file_name, delimiter=\",\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "nu9_D4L85tA2"
      },
      "outputs": [],
      "source": [
        "class HEV:\n",
        "    def __init__(self, fmu_filename, start_time, stop_time, step_size, soc_init, vehicle_speed_profile):\n",
        "        self.fmu_filename = fmu_filename\n",
        "        self.vrs = {}\n",
        "        self.start_time = start_time\n",
        "        self.stop_time = stop_time\n",
        "        self.step_size = step_size\n",
        "        self.time = self.start_time\n",
        "        self.vehicle_speed_profile = vehicle_speed_profile\n",
        "        self.soc_init = soc_init/5000\n",
        "        self.state_init = np.array([self.soc_init, 0, 0, 0, 0, 0, 0, 0, 0]).reshape(1,-1)\n",
        "        self.state = np.array([self.soc_init, 0, 0, 0, 0, 0, 0, 0, 0]).reshape(1,-1)\n",
        "        self.action_upper_bound = 15000\n",
        "        self.action_lower_bound = -15000\n",
        "        #self.action_space = [[13500, 2000], [500, 2000], [13500, -15000], [500, -15000]]\n",
        "        self.obssize = len(self.state[0])\n",
        "        self.actsize = 2\n",
        "\n",
        "    def step(self, action):\n",
        "        #a = self.action_space[action]\n",
        "#         action = action*self.action_upper_bound\n",
        "        a1 = action[0]*self.action_upper_bound\n",
        "        a2 = action[0]*self.action_upper_bound\n",
        "        a3 = self.soc_init\n",
        "        a4 = action[1]/10 + 1\n",
        "        instant_veh_speed = np.interp(self.time, self.vehicle_speed_profile[:,0], self.vehicle_speed_profile[:,1])\n",
        "        self.fmu.setReal([self.vr_input1, self.vr_input2, self.vr_input3, self.vr_input4, self.vr_input5], [instant_veh_speed, a1, a2, a3, a4]) #input variable, input key(13500 2000)\n",
        "        self.fmu.doStep(currentCommunicationPoint=self.time, communicationStepSize=self.step_size)\n",
        "#         [output1, output2, output3, output4, output5, output6, output7] = self.fmu.getReal([self.vr_output1,self.vr_output2, self.vr_output3, self.vr_output4, self.vr_output5, self.vr_output6, self.vr_output7])\n",
        "#         self.state = [output1, output2, output3, output4, output5, output6, output7]\n",
        "        state = np.array(self.fmu.getReal(np.arange(35)))/5000\n",
        "        state_column = np.array([self.vrs['Bat_SOC'], self.vrs['nEngOn'], self.vrs['PT_tqTmInDmd_Nm_P2'], self.vrs['ObEng_nEng_Rpm'], self.vrs['TrEtp_tqEngMAF_Nm'], self.vrs['TrP0_tqP0_Nm'], self.vrs['TrP2_tqP2_Nm'], self.vrs['Driver_sVeh_kph'], self.vrs['BSFC_g_kWh[1]']])\n",
        "        self.state = state[state_column]\n",
        "        soc = state[self.vrs['Bat_SOC']]\n",
        "        BSFC = state[self.vrs['BSFC_g_kWh[1]']]\n",
        "        reward = - 5000*(self.soc_init - soc) ** 2 - 10*BSFC\n",
        "        is_done = lambda time: time >= self.stop_time\n",
        "        self.time += self.step_size\n",
        "        return self.state.reshape(1,-1), reward, is_done(self.time), None\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = self.state_init\n",
        "        model_description = read_model_description(self.fmu_filename)\n",
        "        for variable in model_description.modelVariables:\n",
        "            self.vrs[variable.name] = variable.valueReference\n",
        "        unzipdir = extract(fmu_filename)\n",
        "        self.fmu = FMU2Slave(guid=model_description.guid,\n",
        "                       unzipDirectory=unzipdir,\n",
        "                       modelIdentifier=model_description.coSimulation.modelIdentifier,\n",
        "                       instanceName='instance1')\n",
        "        self.fmu.instantiate()\n",
        "        self.fmu.setupExperiment(startTime=self.start_time)\n",
        "        self.fmu.enterInitializationMode()\n",
        "        self.fmu.exitInitializationMode()\n",
        "        self.time = self.start_time\n",
        "\n",
        "        self.vr_input1 = self.vrs['Driver_sVeh_Target_kph']\n",
        "        self.vr_input2 = self.vrs['Engine_on_line']\n",
        "        self.vr_input3 = self.vrs['Engine_off_line']\n",
        "        self.vr_input4 = self.vrs['soc_init']\n",
        "        self.vr_input5 = self.vrs['Engine_OOL']\n",
        "        self.vr_output1 = self.vrs['TgMod_fPt']\n",
        "        self.vr_output2 = self.vrs['P2_wElec_W']\n",
        "        self.vr_output3 = self.vrs['P4_wElec_W']\n",
        "        self.vr_output4 = self.vrs['ObP2_wElecBIntv_W']\n",
        "        self.vr_output5 = self.vrs['EV_on_line']\n",
        "        self.vr_output6 = self.vrs['EV_off_line']\n",
        "        self.vr_output7 = self.vrs['Bat_SOC']\n",
        "        self.vr_output8 = self.vrs['PT_tqTmInDmd_Nm_P2']\n",
        "        self.vr_output9 = self.vrs['P0_wElec_W']\n",
        "        self.vr_output10 = self.vrs['Pwr_Aux_W']\n",
        "        self.vr_output11 = self.vrs['ObEng_nEng_Rpm']\n",
        "        self.vr_output12 = self.vrs['TrEtp_tqEngMAF_Nm']\n",
        "        self.vr_output13 = self.vrs['rpm_P0']\n",
        "        self.vr_output14 = self.vrs['TM_F_nTmIn_rpm']\n",
        "        self.vr_output15 = self.vrs['TM_R_nTmIn_rpm']\n",
        "        self.vr_output16 = self.vrs['eBat_kWh']\n",
        "        self.vr_output17 = self.vrs['nEngOn']\n",
        "        self.vr_output18 = self.vrs['TrP0_tqP0_Nm']\n",
        "        self.vr_output19 = self.vrs['TrP2_tqP2_Nm']\n",
        "        self.vr_output20 = self.vrs['TrP4_tqP4_Nm']\n",
        "        self.vr_output21 = self.vrs['TCU_F_fCurGe']\n",
        "        self.vr_output22 = self.vrs['TCU_F_fTarGe']\n",
        "        self.vr_output23 = self.vrs['Driver_sVeh_kph']\n",
        "        self.vr_output24 = self.vrs['Eng_eff_avg']\n",
        "        self.vr_output25 = self.vrs['TM_F_P0P2_eff_avg']\n",
        "        self.vr_output26 = self.vrs['TM_F_P0P4_eff_avg']\n",
        "        self.vr_output27 = self.vrs['BSFC_g_kWh[1]']\n",
        "        self.vr_output28 = self.vrs['BSFC_g_kWh[2]']\n",
        "        self.vr_output29 = self.vrs['BSFC_g_kWh[3]']\n",
        "\n",
        "        return self.state.reshape(1,-1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "HAZ8kZoO5-x-",
        "outputId": "a9376a86-4af1-4db2-98ed-dea37b3abbb4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nenv = HEV(fmu_filename, start_time, stop_time, step_size, soc_init, vehicle_speed_profile)\\nenv.reset()\\n\\ndone = False\\ncum_reward = 0.0\\nEngLine = pd.read_csv(\\'wltp_EngLine_rule_based_10Hz.csv\\')\\nEngLine = np.array(EngLine[\\'EngLine\\'])\\n# action = np.array([13500/15000])\\nstart = time.time()\\nsimulink_r = []\\nsimulink_s = []\\nt = 0\\nwhile not done:\\n    line = EngLine[t]\\n    t += 1\\n    state, reward, done, _ = env.step(np.array([line,0]))\\n    cum_reward += reward\\n    simulink_r.append(reward)\\n    simulink_s.append(state)\\nprint(time.time()-start) # 21.4921\\nprint(f\"total reward: {cum_reward}\") #-114122.92538532468\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "'''\n",
        "env = HEV(fmu_filename, start_time, stop_time, step_size, soc_init, vehicle_speed_profile)\n",
        "env.reset()\n",
        "\n",
        "done = False\n",
        "cum_reward = 0.0\n",
        "EngLine = pd.read_csv('wltp_EngLine_rule_based_10Hz.csv')\n",
        "EngLine = np.array(EngLine['EngLine'])\n",
        "# action = np.array([13500/15000])\n",
        "start = time.time()\n",
        "simulink_r = []\n",
        "simulink_s = []\n",
        "t = 0\n",
        "while not done:\n",
        "    line = EngLine[t]\n",
        "    t += 1\n",
        "    state, reward, done, _ = env.step(np.array([line,0]))\n",
        "    cum_reward += reward\n",
        "    simulink_r.append(reward)\n",
        "    simulink_s.append(state)\n",
        "print(time.time()-start) # 21.4921\n",
        "print(f\"total reward: {cum_reward}\") #-114122.92538532468\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Buffer"
      ],
      "metadata": {
        "id": "vMKRPZjw8VWo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BasicBuffer:\n",
        "\n",
        "  def __init__(self, max_size):\n",
        "      self.max_size = max_size\n",
        "      self.buffer = deque(maxlen=max_size)\n",
        "\n",
        "  def push(self, state, action, reward, next_state, done):\n",
        "      experience = (state, action, np.array([reward]), next_state, done)\n",
        "      self.buffer.append(experience)\n",
        "\n",
        "  def sample(self, batch_size):\n",
        "      state_batch = []\n",
        "      action_batch = []\n",
        "      reward_batch = []\n",
        "      next_state_batch = []\n",
        "      done_batch = []\n",
        "\n",
        "      batch = random.sample(self.buffer, batch_size)\n",
        "\n",
        "      for experience in batch:\n",
        "          state, action, reward, next_state, done = experience\n",
        "          state_batch.append(state)\n",
        "          action_batch.append(action)\n",
        "          reward_batch.append(reward)\n",
        "          next_state_batch.append(next_state)\n",
        "          done_batch.append(done)\n",
        "\n",
        "      return (state_batch, action_batch, reward_batch, next_state_batch, done_batch)\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.buffer)"
      ],
      "metadata": {
        "id": "vmwl-dtm8XCH"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ValueNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, init_w=3e-3):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, 256)\n",
        "        self.fc2 = nn.Linear(256, 256)\n",
        "        self.fc3 = nn.Linear(256, output_dim)\n",
        "\n",
        "        self.fc3.weight.data.uniform_(-init_w, init_w)\n",
        "        self.fc3.bias.data.uniform_(-init_w, init_w)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "JMOv1qLv8eSH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SoftQNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, num_inputs, num_actions, hidden_size=256, init_w=3e-3):\n",
        "        super(SoftQNetwork, self).__init__()\n",
        "        self.linear1 = nn.Linear(num_inputs + num_actions, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
        "        self.linear3 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "        self.linear3.weight.data.uniform_(-init_w, init_w)\n",
        "        self.linear3.bias.data.uniform_(-init_w, init_w)\n",
        "\n",
        "    def forward(self, state, action):\n",
        "        x = torch.cat([state.squeeze(), action.squeeze()], 1)\n",
        "        x = F.relu(self.linear1(x))\n",
        "        x = F.relu(self.linear2(x))\n",
        "        x = self.linear3(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "gvjMAj1F8fBn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "\n",
        "    def __init__(self, num_inputs, num_actions, hidden_size=256, init_w=3e-3, log_std_min=-20, log_std_max=2):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.log_std_min = log_std_min\n",
        "        self.log_std_max = log_std_max\n",
        "\n",
        "        self.linear1 = nn.Linear(num_inputs, hidden_size)\n",
        "        self.linear2 = nn.Linear(hidden_size, hidden_size)\n",
        "\n",
        "        self.mean_linear = nn.Linear(hidden_size, num_actions)\n",
        "        self.mean_linear.weight.data.uniform_(-init_w, init_w)\n",
        "        self.mean_linear.bias.data.uniform_(-init_w, init_w)\n",
        "\n",
        "        self.log_std_linear = nn.Linear(hidden_size, num_actions)\n",
        "        self.log_std_linear.weight.data.uniform_(-init_w, init_w)\n",
        "        self.log_std_linear.bias.data.uniform_(-init_w, init_w)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = F.relu(self.linear1(state))\n",
        "        x = F.relu(self.linear2(x))\n",
        "\n",
        "        mean    = self.mean_linear(x)\n",
        "        log_std = self.log_std_linear(x)\n",
        "        log_std = torch.clamp(log_std, self.log_std_min, self.log_std_max)\n",
        "\n",
        "        return mean, log_std\n",
        "\n",
        "    def sample(self, state, epsilon=1e-6):\n",
        "        mean, log_std = self.forward(state)\n",
        "        std = log_std.exp()\n",
        "\n",
        "        normal = Normal(mean, std)\n",
        "        z = normal.rsample()\n",
        "        action = torch.tanh(z)\n",
        "\n",
        "        log_pi = normal.log_prob(z) - torch.log(1 - action.pow(2) + epsilon)\n",
        "        log_pi = log_pi.sum(1, keepdim=True)\n",
        "\n",
        "        return action, log_pi"
      ],
      "metadata": {
        "id": "sXtst6Gj8iHO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAC"
      ],
      "metadata": {
        "id": "SPHms3UN8pUg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SACAgent:\n",
        "\n",
        "    def __init__(self, env, gamma, tau, alpha, q_lr, policy_lr, a_lr, buffer_maxlen):\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.env = env\n",
        "        self.action_range = [-1, 1]\n",
        "\n",
        "        # hyperparameters\n",
        "        self.gamma = gamma\n",
        "        self.tau = tau\n",
        "        self.update_step = 0\n",
        "        self.delay_step = 2\n",
        "\n",
        "        # initialize networks\n",
        "        self.q_net1 = SoftQNetwork(env.obssize, env.actsize).to(self.device)\n",
        "        self.q_net2 = SoftQNetwork(env.obssize, env.actsize).to(self.device)\n",
        "        self.target_q_net1 = SoftQNetwork(env.obssize, env.actsize).to(self.device)\n",
        "        self.target_q_net2 = SoftQNetwork(env.obssize, env.actsize).to(self.device)\n",
        "        self.policy_net = PolicyNetwork(env.obssize, env.actsize).to(self.device)\n",
        "\n",
        "        # copy params to target param\n",
        "        for target_param, param in zip(self.target_q_net1.parameters(), self.q_net1.parameters()):\n",
        "            target_param.data.copy_(param)\n",
        "\n",
        "        for target_param, param in zip(self.target_q_net2.parameters(), self.q_net2.parameters()):\n",
        "            target_param.data.copy_(param)\n",
        "\n",
        "        # initialize optimizers\n",
        "        self.q1_optimizer = optim.Adam(self.q_net1.parameters(), lr=q_lr)\n",
        "        self.q2_optimizer = optim.Adam(self.q_net2.parameters(), lr=q_lr)\n",
        "        self.policy_optimizer = optim.Adam(self.policy_net.parameters(), lr=policy_lr)\n",
        "\n",
        "        # entropy temperature\n",
        "        self.alpha = alpha\n",
        "        self.target_entropy = -torch.prod(torch.Tensor(self.env.actsize).to(self.device)).item()\n",
        "        self.log_alpha = torch.zeros(1, requires_grad=True, device=self.device)\n",
        "        self.alpha_optim = optim.Adam([self.log_alpha], lr=a_lr)\n",
        "\n",
        "        self.replay_buffer = BasicBuffer(buffer_maxlen)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
        "        mean, log_std = self.policy_net.forward(state)\n",
        "        std = log_std.exp()\n",
        "\n",
        "        normal = Normal(mean, std)\n",
        "        z = normal.sample()\n",
        "        action = torch.tanh(z)\n",
        "        action = action.cpu().detach().squeeze().numpy()\n",
        "\n",
        "        return self.rescale_action(action)\n",
        "\n",
        "    def rescale_action(self, action):\n",
        "        return action * (self.action_range[1] - self.action_range[0]) / 2.0 +\\\n",
        "            (self.action_range[1] + self.action_range[0]) / 2.0\n",
        "\n",
        "    def update(self, batch_size):\n",
        "        states, actions, rewards, next_states, dones = self.replay_buffer.sample(batch_size)\n",
        "        states = torch.FloatTensor(states).to(self.device)\n",
        "        actions = torch.FloatTensor(actions).to(self.device)\n",
        "        rewards = torch.FloatTensor(rewards).to(self.device)\n",
        "        next_states = torch.FloatTensor(next_states).to(self.device)\n",
        "        dones = torch.FloatTensor(dones).to(self.device)\n",
        "        dones = dones.view(dones.size(0), -1)\n",
        "\n",
        "        next_actions, next_log_pi = self.policy_net.sample(next_states)\n",
        "        next_q1 = self.target_q_net1(next_states, next_actions)\n",
        "        next_q2 = self.target_q_net2(next_states, next_actions)\n",
        "        next_q_target = torch.min(next_q1, next_q2) - self.alpha * next_log_pi\n",
        "        expected_q = rewards + (1 - dones) * self.gamma * next_q_target\n",
        "\n",
        "        # q loss\n",
        "        curr_q1 = self.q_net1.forward(states, actions)\n",
        "        curr_q2 = self.q_net2.forward(states, actions)\n",
        "        q1_loss = F.mse_loss(curr_q1, expected_q.detach())\n",
        "        q2_loss = F.mse_loss(curr_q2, expected_q.detach())\n",
        "\n",
        "        # update q networks\n",
        "        self.q1_optimizer.zero_grad()\n",
        "        q1_loss.backward()\n",
        "        self.q1_optimizer.step()\n",
        "\n",
        "        self.q2_optimizer.zero_grad()\n",
        "        q2_loss.backward()\n",
        "        self.q2_optimizer.step()\n",
        "\n",
        "        # delayed update for policy network and target q networks\n",
        "        new_actions, log_pi = self.policy_net.sample(states)\n",
        "        if self.update_step % self.delay_step == 0:\n",
        "            min_q = torch.min(\n",
        "                self.q_net1.forward(states, new_actions),\n",
        "                self.q_net2.forward(states, new_actions)\n",
        "            )\n",
        "            policy_loss = (self.alpha * log_pi - min_q).mean()\n",
        "\n",
        "            self.policy_optimizer.zero_grad()\n",
        "            policy_loss.backward()\n",
        "            self.policy_optimizer.step()\n",
        "\n",
        "            # target networks\n",
        "            for target_param, param in zip(self.target_q_net1.parameters(), self.q_net1.parameters()):\n",
        "                target_param.data.copy_(self.tau * param + (1 - self.tau) * target_param)\n",
        "\n",
        "            for target_param, param in zip(self.target_q_net2.parameters(), self.q_net2.parameters()):\n",
        "                target_param.data.copy_(self.tau * param + (1 - self.tau) * target_param)\n",
        "\n",
        "        # update temperature\n",
        "        alpha_loss = (self.log_alpha * (-log_pi - self.target_entropy).detach()).mean()\n",
        "\n",
        "        self.alpha_optim.zero_grad()\n",
        "        alpha_loss.backward()\n",
        "        self.alpha_optim.step()\n",
        "        self.alpha = self.log_alpha.exp()\n",
        "\n",
        "        self.update_step += 1"
      ],
      "metadata": {
        "id": "eynTLznV8qQI"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train"
      ],
      "metadata": {
        "id": "s6ttR2p68ukD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mini_batch_train(env, agent, max_episodes, max_steps, batch_size):\n",
        "    episode_rewards = []\n",
        "    update_step = 0\n",
        "\n",
        "    for episode in range(max_episodes):\n",
        "        state = env.reset()\n",
        "        episode_reward = 0\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            print('\\r',round(step*100/max_steps, 2), end = \"\")\n",
        "            action = agent.get_action(state)\n",
        "            next_state, reward, done, _ = env.step(action)\n",
        "            agent.replay_buffer.push(state, action, reward, next_state, done)\n",
        "            episode_reward += reward\n",
        "\n",
        "            if len(agent.replay_buffer) > batch_size:\n",
        "                agent.update(batch_size)\n",
        "                update_step += 1\n",
        "\n",
        "            if done or step == max_steps-1:\n",
        "                episode_rewards.append(episode_reward)\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        print(\"Episode \" + str(episode) + \": \" + str(episode_reward))\n",
        "\n",
        "    return episode_rewards"
      ],
      "metadata": {
        "id": "0YDn0Z1X8vZr"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main"
      ],
      "metadata": {
        "id": "xj4XWlKL8xaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = HEV(fmu_filename, start_time, stop_time, step_size, soc_init, vehicle_speed_profile)\n",
        "\n",
        "# SAC 2019 Params\n",
        "gamma = 0.99\n",
        "tau = 0.01\n",
        "alpha = 0.2\n",
        "a_lr = 3e-4\n",
        "q_lr = 3e-4\n",
        "p_lr = 3e-4\n",
        "buffer_maxlen = int(2e8)\n",
        "\n",
        "# 2019 agent\n",
        "agent = SACAgent(env, gamma, tau, alpha, q_lr, p_lr, a_lr, buffer_maxlen)\n",
        "\n",
        "# train\n",
        "episode_rewards = mini_batch_train(env, agent, 50, 180001, 64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fB66TQ2W8yU7",
        "outputId": "dab800e0-d393-4d60-88f8-b67e03fe4d35"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 0.04"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-bfa59b152270>:60: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
            "  states = torch.FloatTensor(states).to(self.device)\n",
            "<ipython-input-14-bfa59b152270>:76: UserWarning: Using a target size (torch.Size([64, 64, 2])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  q1_loss = F.mse_loss(curr_q1, expected_q.detach())\n",
            "<ipython-input-14-bfa59b152270>:77: UserWarning: Using a target size (torch.Size([64, 64, 2])) that is different to the input size (torch.Size([64, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  q2_loss = F.mse_loss(curr_q2, expected_q.detach())\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " 100.0Episode 0: -80319.9312617085\n",
            " 24.47"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}