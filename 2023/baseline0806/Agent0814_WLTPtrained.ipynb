{"cells":[{"cell_type":"markdown","metadata":{"id":"8633a3c6"},"source":["```\n","random profile에 대하여 학습하는 stochastic PPO agent\n","profile 길이가 1800초 넘어갈때도 있음\n","buffer를 두번 호출하면 에러남\n","\n","log_dir 추가\n","wltp profile로만 학습해봄\n","\n","state 차원 축소\n","reward 계수 조정\n","state 값 1근처로\n","\n","buffer 내에서 normalization 해야할수도\n","\n","tanh 모두 제거\n","\n","SoC reward만\n","state*1000\n","reward/10000\n","\n","```"],"id":"8633a3c6"},{"cell_type":"markdown","metadata":{"id":"46b82727"},"source":["# Import"],"id":"46b82727"},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3701,"status":"ok","timestamp":1692002868124,"user":{"displayName":"­서성민 / 학생 / 기계공학부","userId":"08487562865148337619"},"user_tz":-540},"id":"GyuFQkGvlakO","outputId":"b514f510-9126-4fae-c1f5-840c36ddcf92"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/', force_remount=True)"],"id":"GyuFQkGvlakO"},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1692002868124,"user":{"displayName":"­서성민 / 학생 / 기계공학부","userId":"08487562865148337619"},"user_tz":-540},"id":"_Ku4H_allaZB","outputId":"a92ddaaf-1d3d-47df-a8d7-e0dd693408d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/기계시스템설계/23년/baseline0806\n"]}],"source":["%cd /content/drive/MyDrive/Colab Notebooks/기계시스템설계/23년/baseline0806"],"id":"_Ku4H_allaZB"},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7558,"status":"ok","timestamp":1692002875679,"user":{"displayName":"­서성민 / 학생 / 기계공학부","userId":"08487562865148337619"},"user_tz":-540},"id":"faJq2y_ylhne","outputId":"b6364ba5-d66b-461f-8dbf-16040f34bb7c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: fmpy in /usr/local/lib/python3.10/dist-packages (0.3.15)\n","Requirement already satisfied: attrs in /usr/local/lib/python3.10/dist-packages (from fmpy) (23.1.0)\n","Requirement already satisfied: Jinja2 in /usr/local/lib/python3.10/dist-packages (from fmpy) (3.1.2)\n","Requirement already satisfied: lark in /usr/local/lib/python3.10/dist-packages (from fmpy) (1.1.7)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from fmpy) (4.9.3)\n","Requirement already satisfied: msgpack in /usr/local/lib/python3.10/dist-packages (from fmpy) (1.0.5)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fmpy) (1.23.5)\n","Requirement already satisfied: pytz in /usr/local/lib/python3.10/dist-packages (from fmpy) (2023.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2->fmpy) (2.1.3)\n"]}],"source":["!pip install fmpy"],"id":"faJq2y_ylhne"},{"cell_type":"code","execution_count":4,"metadata":{"id":"3e3fd07c","executionInfo":{"status":"ok","timestamp":1692002877062,"user_tz":-540,"elapsed":1393,"user":{"displayName":"­서성민 / 학생 / 기계공학부","userId":"08487562865148337619"}}},"outputs":[],"source":["import os\n","import sys\n","from glob import glob\n","from fmpy import read_model_description, extract\n","from fmpy.fmi2 import FMU2Slave\n","from fmpy.util import plot_result, download_test_file\n","import numpy as np\n","import pandas as pd\n","import shutil\n","import matplotlib.pyplot as plt\n","import random\n","import argparse\n","from collections import deque\n","from tqdm import tqdm\n","import scipy.signal\n","import time\n","import random\n","import copy"],"id":"3e3fd07c"},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1692002877063,"user":{"displayName":"­서성민 / 학생 / 기계공학부","userId":"08487562865148337619"},"user_tz":-540},"id":"284484b7","outputId":"4398d21f-be89-465f-9765-0b80b74fc2a0"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/기계시스템설계/23년/baseline0806\n"]}],"source":["cwd = os.getcwd()\n","print(cwd)"],"id":"284484b7"},{"cell_type":"code","execution_count":6,"metadata":{"id":"d70e16b5","executionInfo":{"status":"ok","timestamp":1692002879714,"user_tz":-540,"elapsed":2654,"user":{"displayName":"­서성민 / 학생 / 기계공학부","userId":"08487562865148337619"}}},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch import nn, optim\n","from torch.distributions.normal import Normal\n","from torch import FloatTensor as FT\n","\n","import torch\n","import torchvision\n","import torch.nn.functional as F\n","from torch import nn, optim\n","from torchvision import transforms\n","from torch.autograd import Variable\n","from torch.utils.data import Dataset, TensorDataset, DataLoader"],"id":"d70e16b5"},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1692002879715,"user":{"displayName":"­서성민 / 학생 / 기계공학부","userId":"08487562865148337619"},"user_tz":-540},"id":"3abae912","outputId":"7d43e6f4-95fc-4ebb-d1a4-2d0ca99d537d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using Device: cuda:0\n"]}],"source":["# GPU 준비\n","USE_CUDA = torch.cuda.is_available()\n","dev = torch.device(\"cuda:0\" if USE_CUDA else \"cpu\")\n","print(\"Using Device:\", dev)\n","torch.cuda.empty_cache()\n","#dev = torch.device(\"cpu\")"],"id":"3abae912"},{"cell_type":"markdown","metadata":{"id":"f3724d83"},"source":["# HEV Environment"],"id":"f3724d83"},{"cell_type":"code","execution_count":8,"metadata":{"id":"dcc1fa6f","executionInfo":{"status":"ok","timestamp":1692002879715,"user_tz":-540,"elapsed":4,"user":{"displayName":"­서성민 / 학생 / 기계공학부","userId":"08487562865148337619"}}},"outputs":[],"source":["# data_dir = \"D:\\RDEDB_Stat_TripData\\TripDataTintrWithExh\"\n","# def make_profile():\n","#     file_names = []\n","#     folder_names = glob(data_dir + \"\\\\*GI\")\n","#     for folder_name in folder_names:\n","#         for file_name in glob(folder_name+\"/*.csv\"):\n","#             file_names.append(file_name)\n","#     files_num = len(file_names)\n","#     done =True\n","#     while done:\n","#         file_num = random.randint(0, files_num)\n","#         file_name = file_names[file_num]\n","#         profile = np.array(pd.read_csv(file_name)['vehSpeed'])\n","#         done = profile.shape[0]==0\n","#     print(f'Profile Name : {file_name},\\nProfile Time : {profile.shape[0]}')\n","#     return profile\n","# profile = make_profile()\n","# profile.shape"],"id":"dcc1fa6f"},{"cell_type":"code","execution_count":9,"metadata":{"id":"4c90dd2d","executionInfo":{"status":"ok","timestamp":1692002879715,"user_tz":-540,"elapsed":4,"user":{"displayName":"­서성민 / 학생 / 기계공학부","userId":"08487562865148337619"}}},"outputs":[],"source":["# fmu_filename = 'HEV_TMED_Simulator_Rearrange_230213_linux.fmu'\n","fmu_filename = 'HEV_TMED_Simulator_Rearrange_230711_linux.fmu'\n","start_time = 0.0\n","stop_time = 1800.0\n","step_size = 0.01\n","soc_init = 67\n","profile_name = 'wltp_1Hz.csv'\n","# with open(\"wltp_vehicle_speed_profile_real.csv\") as file_name:\n","#     vehicle_speed_profile = np.loadtxt(file_name, delimiter=\",\")"],"id":"4c90dd2d"},{"cell_type":"code","execution_count":10,"metadata":{"id":"6714602a","executionInfo":{"status":"ok","timestamp":1692002879715,"user_tz":-540,"elapsed":3,"user":{"displayName":"­서성민 / 학생 / 기계공학부","userId":"08487562865148337619"}}},"outputs":[],"source":["class HEV:\n","    def __init__(self, fmu_filename, test=False, start_time=0.0, step_size=0.01, SoC_coeff=5, BSFC_coeff=0.5, reward_coeff=1, state_coeff=1, alive_reward=1):\n","        self.fmu_filename = fmu_filename\n","        self.vrs = {}\n","        self.start_time = start_time\n","        self.step_size = step_size\n","        self.SoC_coeff = SoC_coeff\n","        self.BSFC_coeff = BSFC_coeff\n","        self.reward_coeff = reward_coeff\n","        self.state_coeff = state_coeff\n","        self.alive_reward = alive_reward\n","        self.time = self.start_time\n","        self.test = test\n","        if self.test:\n","            self.vehicle_speed_profile =  np.array(pd.read_csv(profile_name))[:,0]\n","            self.soc_init = 67/100\n","            print(f'Initial SoC : {self.soc_init*100}')\n","        else:\n","            self.vehicle_speed_profile = make_profile()\n","            self.soc_init = random.uniform(57, 77)/100\n","            print(f'Initial SoC : {self.soc_init*100}')\n","        self.time_profile = np.arange(self.vehicle_speed_profile.shape[0])\n","        self.stop_time = self.vehicle_speed_profile.shape[0] - 1\n","        self.state_init = np.array([self.soc_init, 0, 0, 0, 0, 0, 0]).reshape(1,-1)\n","        self.soc_base = 67/100\n","        self.state = self.state_init\n","        self.action_upper_bound = 20000\n","        self.action_lower_bound = -20000\n","        #self.action_space = [[13500, 2000], [500, 2000], [13500, -15000], [500, -15000]]\n","        self.obssize = len(self.state[0])\n","        self.actsize = 2\n","        model_description = read_model_description(self.fmu_filename)\n","        for variable in model_description.modelVariables:\n","            self.vrs[variable.name] = variable.valueReference\n","        unzipdir = extract(fmu_filename)\n","        self.fmu = FMU2Slave(guid=model_description.guid,\n","                       unzipDirectory=unzipdir,\n","                       modelIdentifier=model_description.coSimulation.modelIdentifier,\n","                       instanceName='instance1')\n","\n","    def step(self, action):\n","        #a = self.action_space[action]\n","#         action = action*self.action_upper_bound\n","        action = torch.clamp(action, -1.0, 1.0)[0]\n","        a1 = action[0]*self.action_upper_bound\n","        a2 = action[0]*self.action_upper_bound\n","        a3 = self.soc_init*100\n","        a4 = action[1]/2 + 1\n","        instant_veh_speed = np.interp(self.time, self.time_profile, self.vehicle_speed_profile)\n","        self.fmu.setReal([self.vr_input1, self.vr_input2, self.vr_input3, self.vr_input4, self.vr_input5], [instant_veh_speed, a1, a2, a3, a4]) #input variable, input key(13500 2000)\n","        self.fmu.doStep(currentCommunicationPoint=self.time, communicationStepSize=self.step_size)\n","#         [output1, output2, output3, output4, output5, output6, output7] = self.fmu.getReal([self.vr_output1,self.vr_output2, self.vr_output3, self.vr_output4, self.vr_output5, self.vr_output6, self.vr_output7])\n","#         self.state = [output1, output2, output3, output4, output5, output6, output7]\n","        state = np.array(self.fmu.getReal(np.arange(35)))\n","#         state_column = np.array([self.vrs['Bat_SOC'], self.vrs['nEngOn'], self.vrs['PT_tqTmInDmd_Nm_P2'], self.vrs['ObEng_nEng_Rpm'], self.vrs['TrEtp_tqEngMAF_Nm'], self.vrs['TrP0_tqP0_Nm'], self.vrs['TrP2_tqP2_Nm'], self.vrs['Driver_sVeh_kph'], self.vrs['BSFC_g_kWh[1]']])\n","        state_column = np.array([self.vrs['Bat_SOC'], self.vrs['BSFC_g_kWh[1]'], self.vrs['ObEng_nEng_Rpm'], self.vrs['TrEtp_tqEngMAF_Nm'], self.vrs['TrP0_tqP0_Nm'], self.vrs['TrP2_tqP2_Nm'], self.vrs['Driver_sVeh_kph']])\n","        self.state = state[state_column]\n","        soc = state[self.vrs['Bat_SOC']]/100\n","        BSFC = state[self.vrs['BSFC_g_kWh[1]']]/300\n","        engine_speed = state[self.vrs['ObEng_nEng_Rpm']]/2500\n","        engine_torque = state[self.vrs['TrEtp_tqEngMAF_Nm']]/250\n","        P0_torque = state[self.vrs['TrP0_tqP0_Nm']]/30\n","        P2_torque = state[self.vrs['TrP2_tqP2_Nm']]/200\n","        vehicle_speed = state[self.vrs['Driver_sVeh_kph']]/100\n","        self.state = np.array([soc, BSFC, engine_speed, engine_torque, P0_torque, P2_torque, vehicle_speed])\n","\n","        reward = self.alive_reward - self.SoC_coeff * (self.soc_base - soc) ** 2 - self.BSFC_coeff * BSFC\n","        reward = self.reward_coeff * reward\n","        is_done = lambda time: time >= self.stop_time\n","        info = state[np.array([self.vrs['Bat_SOC'], self.vrs['nEngOn'], self.vrs['PT_tqTmInDmd_Nm_P2'], self.vrs['ObEng_nEng_Rpm'], self.vrs['TrEtp_tqEngMAF_Nm'], self.vrs['TrP0_tqP0_Nm'], self.vrs['TrP2_tqP2_Nm'], self.vrs['Driver_sVeh_kph'], self.vrs['BSFC_g_kWh[1]']])]\n","        self.time += self.step_size\n","        return self.state.reshape(1,-1)*self.state_coeff, reward, is_done(self.time), info\n","\n","    def reset(self):\n","        self.fmu.instantiate()\n","        self.fmu.setupExperiment(startTime=self.start_time)\n","        self.fmu.enterInitializationMode()\n","        self.fmu.exitInitializationMode()\n","        self.state = self.state_init\n","        self.time = self.start_time\n","        if self.test:\n","            self.vehicle_speed_profile =  np.array(pd.read_csv(profile_name))[:,0]\n","            self.soc_init = 67/100\n","            print(f'Initial SoC : {self.soc_init*100}')\n","        else:\n","            self.vehicle_speed_profile = make_profile()\n","            self.soc_init = random.uniform(57, 77)/100\n","            print(f'Initial SoC : {self.soc_init*100}')\n","        self.time_profile = np.arange(self.vehicle_speed_profile.shape[0])\n","        self.stop_time = self.vehicle_speed_profile.shape[0] - 1\n","\n","        self.vr_input1 = self.vrs['Driver_sVeh_Target_kph']\n","        self.vr_input2 = self.vrs['Engine_on_line']\n","        self.vr_input3 = self.vrs['Engine_off_line']\n","        self.vr_input4 = self.vrs['soc_init']\n","        self.vr_input5 = self.vrs['Engine_OOL']\n","        self.vr_output1 = self.vrs['TgMod_fPt']\n","        self.vr_output2 = self.vrs['P2_wElec_W']\n","        self.vr_output3 = self.vrs['P4_wElec_W']\n","        self.vr_output4 = self.vrs['ObP2_wElecBIntv_W']\n","        self.vr_output5 = self.vrs['EV_on_line']\n","        self.vr_output6 = self.vrs['EV_off_line']\n","        self.vr_output7 = self.vrs['Bat_SOC']\n","        self.vr_output8 = self.vrs['PT_tqTmInDmd_Nm_P2']\n","        self.vr_output9 = self.vrs['P0_wElec_W']\n","        self.vr_output10 = self.vrs['Pwr_Aux_W']\n","        self.vr_output11 = self.vrs['ObEng_nEng_Rpm']\n","        self.vr_output12 = self.vrs['TrEtp_tqEngMAF_Nm']\n","        self.vr_output13 = self.vrs['rpm_P0']\n","        self.vr_output14 = self.vrs['TM_F_nTmIn_rpm']\n","        self.vr_output15 = self.vrs['TM_R_nTmIn_rpm']\n","        self.vr_output16 = self.vrs['eBat_kWh']\n","        self.vr_output17 = self.vrs['nEngOn']\n","        self.vr_output18 = self.vrs['TrP0_tqP0_Nm']\n","        self.vr_output19 = self.vrs['TrP2_tqP2_Nm']\n","        self.vr_output20 = self.vrs['TrP4_tqP4_Nm']\n","        self.vr_output21 = self.vrs['TCU_F_fCurGe']\n","        self.vr_output22 = self.vrs['TCU_F_fTarGe']\n","        self.vr_output23 = self.vrs['Driver_sVeh_kph']\n","        self.vr_output24 = self.vrs['Eng_eff_avg']\n","        self.vr_output25 = self.vrs['TM_F_P0P2_eff_avg']\n","        self.vr_output26 = self.vrs['TM_F_P0P4_eff_avg']\n","        self.vr_output27 = self.vrs['BSFC_g_kWh[1]']\n","        self.vr_output28 = self.vrs['BSFC_g_kWh[2]']\n","        self.vr_output29 = self.vrs['BSFC_g_kWh[3]']\n","\n","        return self.state.reshape(1,-1)"],"id":"6714602a"},{"cell_type":"markdown","metadata":{"id":"be0c58c8"},"source":["# Agent"],"id":"be0c58c8"},{"cell_type":"code","execution_count":11,"metadata":{"id":"1a77f11c","executionInfo":{"status":"ok","timestamp":1692002880208,"user_tz":-540,"elapsed":496,"user":{"displayName":"­서성민 / 학생 / 기계공학부","userId":"08487562865148337619"}}},"outputs":[],"source":["class Generate_Dataset(torch.utils.data.Dataset):\n","    def __init__(self, dev, buffer1, buffer2, buffer3, buffer4, buffer5):\n","        self.buffer1 = FT(buffer1).clone().detach().to(dev)\n","        self.buffer2 = FT(buffer2).clone().detach().to(dev)\n","        self.buffer3 = FT(buffer3).clone().detach().to(dev)\n","        self.buffer4 = FT(buffer4).clone().detach().to(dev)\n","        self.buffer5 = FT(buffer5).clone().detach().to(dev)\n","    def __len__(self):\n","        return len(self.buffer1)\n","    def __getitem__(self, idx):\n","        buffer1 = self.buffer1[idx]\n","        buffer2 = self.buffer2[idx]\n","        buffer3 = self.buffer3[idx]\n","        buffer4 = self.buffer4[idx]\n","        buffer5 = self.buffer5[idx]\n","        return buffer1, buffer2, buffer3, buffer4, buffer5\n","\n","def discounted_cumulative_sums(x, discount):\n","    # Discounted cumulative sums of vectors for computing rewards-to-go and advantage estimates\n","    return torch.flip(torch.tensor(scipy.signal.lfilter([1], [1, float(-discount)], torch.flip(x, dims=[0]).detach().numpy(), axis=0)), dims=[0])\n","\n","class Buffer:\n","    # Buffer for storing trajectories\n","    def __init__(self, observation_dimensions, action_dimensions, size, gamma=0.99, lam=0.95):\n","        # Buffer initialization\n","        self.observation_buffer = torch.zeros((size, observation_dimensions), dtype=torch.float32)\n","        self.action_buffer = torch.zeros((size, action_dimensions), dtype=torch.float32)\n","        self.advantage_buffer = torch.zeros(size, dtype=torch.float32)\n","        self.reward_buffer = torch.zeros(size, dtype=torch.float32)\n","        self.return_buffer = torch.zeros(size, dtype=torch.float32)\n","        self.value_buffer = torch.zeros(size, dtype=torch.float32)\n","        self.logprobability_buffer = torch.zeros((size, action_dimensions), dtype=torch.float32)\n","        self.gamma, self.lam = gamma, lam\n","        self.pointer, self.trajectory_start_index = 0, 0\n","\n","    def store(self, observation, action, reward, value, logprobability):\n","        # Append one step of agent-environment interaction\n","        self.observation_buffer[self.pointer] = FT(observation)\n","        self.action_buffer[self.pointer] = FT(action)\n","        self.reward_buffer[self.pointer] = FT([reward])\n","        self.value_buffer[self.pointer] = FT(value)\n","        self.logprobability_buffer[self.pointer] = FT(logprobability)\n","        self.pointer += 1\n","\n","    def finish_trajectory(self, last_value=0):\n","        # Finish the trajectory by computing advantage estimates and rewards-to-go\n","        path_slice = slice(self.trajectory_start_index, self.pointer)\n","        rewards = torch.cat([self.reward_buffer[path_slice], torch.tensor([last_value])])\n","        values = torch.cat([self.value_buffer[path_slice], torch.tensor([last_value])])\n","\n","        deltas = rewards[:-1] + self.gamma * values[1:] - values[:-1]\n","\n","        self.advantage_buffer[path_slice] = discounted_cumulative_sums(\n","            deltas, self.gamma * self.lam\n","        )\n","        self.return_buffer[path_slice] = discounted_cumulative_sums(\n","            rewards, self.gamma\n","        )[:-1]\n","\n","#         self.trajectory_start_index = self.pointer\n","\n","    def get(self):\n","        # Get all data of the buffer and normalize the advantages\n","        self.path_slice = slice(self.trajectory_start_index, self.pointer)\n","        path_slice = slice(self.trajectory_start_index, self.pointer)\n","        self.pointer, self.trajectory_start_index = 0, 0\n","#         advantage_mean, advantage_std = (\n","#             torch.mean(self.advantage_buffer[path_slice]),\n","#             torch.std(self.advantage_buffer[path_slice]),\n","#         )\n","#         self.advantage_buffer[path_slice] = (self.advantage_buffer[path_slice] - advantage_mean) / advantage_std\n","        # return_mean, return_std = (\n","        #     torch.mean(self.return_buffer[path_slice]),\n","        #     torch.std(self.return_buffer[path_slice]),\n","        # )\n","        # self.return_buffer[path_slice] = (self.return_buffer[path_slice] - return_mean) / return_std\n","        return (\n","            self.observation_buffer[path_slice],\n","            self.action_buffer[path_slice],\n","            self.advantage_buffer[path_slice],\n","            self.return_buffer[path_slice],\n","            self.logprobability_buffer[path_slice],\n","            self.reward_buffer[path_slice]\n","        )\n","\n","\n","class ActorCritic(nn.Module):\n","    def __init__(self, num_states, num_actions):\n","        super(ActorCritic, self).__init__()\n","        self.actorlayer = nn.Sequential(\n","            nn.Linear(num_states, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.Linear(64,32),\n","            nn.ReLU(),)\n","\n","        self.criticlayer = nn.Sequential(\n","            nn.Linear(num_states, 128),\n","            nn.ReLU(),\n","            nn.Linear(128, 64),\n","            nn.ReLU(),\n","            nn.Linear(64,32),\n","            nn.ReLU(),)\n","        self.layer2 = nn.Linear(32, num_actions)\n","        self.layer3 = nn.Linear(32, num_actions)\n","        self.layer4 = nn.Linear(32, 1)\n","\n","    def forward(self, states):\n","        x = self.actorlayer(states)\n","        x2 = self.criticlayer(states)\n","        mu = self.layer2(x)\n","        mu = torch.tanh(mu)\n","        std = self.layer3(x)\n","        std = torch.exp(torch.tanh(std)*11-9)\n","#         std = torch.log(1 + torch.exp(std))\n","        #std = torch.tanh(std)\n","        #std = self.softplus(std)\n","        value = self.layer4(x2)\n","        # value = torch.tanh(value)\n","        return [mu, std], value\n","\n","\n","class PPO_agent(nn.Module):      # PPO agent\n","\n","    def __init__(self, e, log_dir, gamma=0.99, clip_ratio=0.2, actor_lr = 3e-4, critic_lr = 1e-3, alpha=0.01):\n","        super(PPO_agent, self).__init__()\n","        # Hyperparameters of the PPO algorithm\n","        self.dev = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","        self.log_dir = log_dir\n","        self.steps_per_episode = 800000\n","        self.episodes = 100\n","        self.EPOCH = 10\n","        self.gamma = gamma\n","        self.clip_ratio = clip_ratio\n","        self.actor_lr = actor_lr\n","        self.critic_lr = critic_lr\n","        self.alpha = alpha\n","        self.r_sum = 0\n","        self.loss = []\n","\n","        self.lam = 0.97\n","        self.start_episode = 0\n","        self.pretrained_episode = 0\n","\n","        self.env = e\n","        self.observation_dimensions = self.env.obssize\n","        self.num_actions = self.env.actsize\n","\n","        self.buffer = Buffer(self.observation_dimensions, self.num_actions, self.steps_per_episode)\n","        self.actorcritic = ActorCritic(self.observation_dimensions, self.num_actions).to(self.dev)\n","        actorparam = list(self.actorcritic.actorlayer.parameters()) + list(self.actorcritic.layer2.parameters()) + list(self.actorcritic.layer3.parameters())\n","        self.optimizer_actor = torch.optim.Adam(actorparam,lr=self.actor_lr)\n","        criticparam = list(self.actorcritic.criticlayer.parameters()) + list(self.actorcritic.layer4.parameters())\n","        self.optimizer_critic = torch.optim.Adam(criticparam,lr=self.critic_lr)\n","        self.actorcritic_clone = ActorCritic(self.observation_dimensions, self.num_actions)\n","\n","        # self.scheduler = optim.lr_scheduler.LambdaLR(optimizer=self.optimizer_actor,\n","        #                                              lr_lambda=lambda epoch: 0.99 ** (epoch+self.start_epoch),\n","        #                                              last_epoch=-1,\n","        #                                              verbose=False)\n","        print(\"The agent has been created..\")\n","    # Define the action sampling function\n","    def sample_continuous_action(self, observation):\n","        [mu, std], value = self.actorcritic(FT(observation.reshape(1,-1)).to(self.dev))\n","        action_dist = Normal(loc=mu, scale=std)\n","        action = action_dist.sample()\n","        action = torch.clamp(action, -1.0, 1.0)[0]\n","\n","        return action.cpu()\n","\n","    def sample_action(self, observation):\n","        [mu, std], value = self.actorcritic(FT(observation.reshape(1,-1)).to(self.dev))\n","#         action = torch.clamp(mu, -1.0, 1.0)[0]\n","\n","        return mu.cpu().detach()\n","\n","\n","    def update(self,\n","               observation_buffer_cpu, action_buffer_cpu, advantage_buffer_cpu, return_buffer_cpu, logprobability_buffer_cpu):\n","\n","        train_ds = Generate_Dataset(self.dev, observation_buffer_cpu, action_buffer_cpu, advantage_buffer_cpu, return_buffer_cpu, logprobability_buffer_cpu)\n","        train_dl = DataLoader(train_ds, batch_size=256, shuffle=True)\n","\n","        for epoch in range(self.EPOCH):\n","            loss_actor = 0\n","            loss_critic = 0\n","            loss_entropy = 0\n","            with tqdm(train_dl, unit = 'batch') as tepoch:\n","                for observation_buffer, action_buffer, advantage_buffer, return_buffer, logprobability_buffer in tepoch:\n","                    # Calculate new probabilities and ratio\n","                    [mu, std], new_values = self.actorcritic(observation_buffer)\n","\n","                    new_action_dist = Normal(mu, std)\n","                    new_probs = new_action_dist.log_prob(action_buffer)\n","\n","                    ratio = new_probs - logprobability_buffer\n","                    ratio = torch.clamp(ratio, max=50)\n","                    ratio = torch.exp(ratio).mean(1)\n","\n","                    # Calculate surrogate loss\n","                    unclipped_obj = ratio * advantage_buffer\n","                    clipped_obj = torch.clamp(ratio, 1-self.clip_ratio, 1+self.clip_ratio) * advantage_buffer\n","                    actor_loss = -torch.min(unclipped_obj, clipped_obj).sum()\n","\n","                    # Calculate value loss\n","                    critic_loss = F.mse_loss(new_values, return_buffer.unsqueeze(1))\n","\n","                    # Calculate entropy bonus\n","                    entropy_bonus = new_action_dist.entropy().sum()\n","\n","                    # Calculate total loss\n","                    total_loss = actor_loss - self.alpha*entropy_bonus*(1.01**(self.episode*self.EPOCH+epoch))\n","\n","                    # Calculate gradients and update weights\n","                    self.optimizer_actor.zero_grad()\n","                    total_loss.backward()\n","                    self.optimizer_actor.step()\n","\n","                    self.optimizer_critic.zero_grad()\n","                    critic_loss.backward()\n","                    self.optimizer_critic.step()\n","\n","                    loss_actor += actor_loss / (self.steps_per_episode/256)\n","                    loss_critic += critic_loss / (self.steps_per_episode/256)\n","                    loss_entropy += entropy_bonus / (self.steps_per_episode/256)\n","                    tepoch.set_description(f\"Epoch {epoch+1}\")\n","                    tepoch.set_postfix(loss_actor=loss_actor.item(), loss_critic=loss_critic.item(), loss_entropy=loss_entropy.item())\n","                    #                   self.loss.append([total_loss.item(), actor_loss.item(), critic_loss.item(), -self.alpha*entropy_bonus.item()])\n","                  # self.scheduler.step()\n","\n","    def load_weights(self, EPISODE):\n","        print(\"Loading existing model..\")\n","        self.start_episode = EPISODE\n","        self.actorcritic = torch.load(os.getcwd() + self.log_dir + f'/PPO_actorcritic_{self.start_episode}episode.pt')    # load policy\n","\n","    def history(self):\n","        print(\"making episode history..\")\n","        self.actorcritic.eval()\n","        self.env = HEV(fmu_filename, test=True, start_time=0.0, step_size = 0.01)\n","        observation = self.env.reset()\n","        done = False\n","        cum_reward = 0.0\n","        a_record = []\n","        r_record = []\n","        s_record = []\n","        while not done:\n","            observation = observation.reshape(1,-1)\n","            action = self.sample_action(observation)\n","            observation_new, reward, done, info = self.env.step(action)\n","            cum_reward += reward\n","            a_record.append(np.array(action))\n","            r_record.append(reward)\n","            s_record.append(info)\n","            observation = observation_new\n","        print(f\"total reward: {cum_reward}\")\n","        self.r_sum = cum_reward\n","        return np.squeeze(a_record), np.array(r_record), np.squeeze(s_record)\n","\n","    def train(self, EPISODES=10):\n","        self.episodes = EPISODES\n","        self.r_record = []\n","        sum_return = 0\n","        sum_length = 0\n","        num_episodes = 0\n","        print(\"Start training..\")\n","        self.actorcritic.train()\n","\n","        for episode in range(self.episodes):\n","            observation, episode_return, episode_length = self.env.reset(), 0, 0\n","            start = time.time()\n","            self.episode = episode + self.start_episode\n","\n","            self.steps_per_episode = int(env.stop_time / env.step_size)\n","            self.actorcritic_clone = copy.deepcopy(self.actorcritic).cpu()\n","\n","            for t in range(self.steps_per_episode):\n","\n","                # Normal distribution with mean=0, std=1\n","                normal_dist = torch.distributions.Normal(0, 1)\n","                epsilon = normal_dist.sample((2,)).reshape(1,-1)\n","\n","                [mu, std], value_t = self.actorcritic_clone(FT(observation.reshape(1,-1)))\n","#                 action_dist = Normal(loc=mu, scale=std)\n","#                 action = action_dist.sample()\n","                action = mu + std*epsilon\n","                logprobability_t = normal_dist.log_prob(epsilon)\n","\n","                observation_new, reward, done, _ = self.env.step(action)\n","\n","                episode_return += reward\n","                episode_length += 1\n","\n","                self.buffer.store(observation, action, reward, value_t, logprobability_t)\n","\n","                observation = observation_new\n","\n","                terminal = done\n","                if terminal or (t == self.steps_per_episode - 1):\n","                    if done:\n","                        last_value = 0\n","                    else:\n","                        [_, _], last_value = self.actorcritic_clone(FT(observation.reshape(1,-1)))\n","                    self.buffer.finish_trajectory(last_value)\n","                    sum_return += episode_return\n","                    sum_length += episode_length\n","                    num_episodes += 1\n","\n","            (\n","                observation_buffer,\n","                action_buffer,\n","                advantage_buffer,\n","                return_buffer,\n","                logprobability_buffer,\n","                reward_buffer\n","            ) = self.buffer.get()\n","\n","            self.data = [observation_buffer, action_buffer, advantage_buffer, return_buffer, logprobability_buffer, reward_buffer]\n","            self.update(\n","                observation_buffer, action_buffer, advantage_buffer, return_buffer, logprobability_buffer\n","                )\n","\n","            print(\n","                \" Episode: \"+str(self.start_episode + episode + 1)+\". Mean Return: \"+str(sum_return / sum_length)+\". Mean Length: \"+str(sum_length / num_episodes)\n","            )\n","            self.r_record.append(sum_return/sum_length)\n","#             print(self.loss[episode])\n","            print(time.time()-start)\n","            torch.save(self.actorcritic, os.getcwd()+ self.log_dir + f'/PPO_actorcritic_{self.start_episode+episode+1}episode.pt')\n","        return self.r_record, observation_buffer, action_buffer, advantage_buffer, return_buffer, logprobability_buffer, reward_buffer"],"id":"1a77f11c"},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2249,"status":"ok","timestamp":1692002882454,"user":{"displayName":"­서성민 / 학생 / 기계공학부","userId":"08487562865148337619"},"user_tz":-540},"id":"a1e0fcc6","outputId":"6132cf36-de87-4ded-ec55-cf9c4876bc0c","scrolled":true},"outputs":[{"output_type":"stream","name":"stdout","text":["Initial SoC : 67.0\n","Initial SoC : 67.0\n","The agent has been created..\n"]}],"source":["env = HEV(fmu_filename, test=True, start_time=0.0, step_size = 0.01, SoC_coeff=5, BSFC_coeff=0.5, reward_coeff=1, state_coeff=1, alive_reward=2)\n","log_dir = '/model/WLTP_trained'\n","state = env.reset()\n","agent = PPO_agent(env, log_dir, gamma=0.99, clip_ratio=0.2, actor_lr=3e-4, critic_lr=1e-3, alpha=0.01)"],"id":"a1e0fcc6"},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"131677b8","scrolled":true,"outputId":"64fd0998-0e87-409f-aff8-c516a5664e4d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Start training..\n","Initial SoC : 67.0\n"]},{"output_type":"stream","name":"stderr","text":["Epoch 1: 100%|██████████| 704/704 [00:14<00:00, 49.65batch/s, loss_actor=-4.28e+3, loss_critic=2.74e+3, loss_entropy=-2.69e+3]\n","Epoch 2: 100%|██████████| 704/704 [00:10<00:00, 64.14batch/s, loss_actor=-7.42e+3, loss_critic=156, loss_entropy=381]\n","Epoch 3: 100%|██████████| 704/704 [00:11<00:00, 62.18batch/s, loss_actor=-7.42e+3, loss_critic=109, loss_entropy=747]\n","Epoch 4: 100%|██████████| 704/704 [00:11<00:00, 61.67batch/s, loss_actor=-7.42e+3, loss_critic=51.1, loss_entropy=883]\n","Epoch 5: 100%|██████████| 704/704 [00:11<00:00, 61.70batch/s, loss_actor=-7.42e+3, loss_critic=26.1, loss_entropy=945]\n","Epoch 6: 100%|██████████| 704/704 [00:10<00:00, 67.33batch/s, loss_actor=-7.42e+3, loss_critic=23.6, loss_entropy=962]\n","Epoch 7: 100%|██████████| 704/704 [00:11<00:00, 61.99batch/s, loss_actor=-7.42e+3, loss_critic=22.5, loss_entropy=968]\n","Epoch 8: 100%|██████████| 704/704 [00:11<00:00, 62.78batch/s, loss_actor=-7.42e+3, loss_critic=21.8, loss_entropy=981]\n","Epoch 9: 100%|██████████| 704/704 [00:11<00:00, 61.68batch/s, loss_actor=-7.42e+3, loss_critic=21.1, loss_entropy=983]\n","Epoch 10: 100%|██████████| 704/704 [00:10<00:00, 67.79batch/s, loss_actor=-7.42e+3, loss_critic=20.6, loss_entropy=986]\n"]},{"output_type":"stream","name":"stdout","text":[" Episode: 1. Mean Return: 1.4372039247042268. Mean Length: 180000.0\n","321.64032435417175\n","Initial SoC : 67.0\n"]}],"source":["#training agent\n","episode = 20\n","# agent.load_weights(20)\n","AUC, observation_buffer, action_buffer, advantage_buffer, return_buffer, logprobability_buffer, reward_buffer = agent.train(20)\n","\n","plt.plot(AUC, label = 'stochastic PPO agent')\n","plt.legend()\n","plt.xlabel('episodes')\n","plt.ylabel('total reward')\n","plt.title('HEV')\n","plt.show()"],"id":"131677b8"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b9ddfbab"},"outputs":[],"source":["agent.loss"],"id":"b9ddfbab"},{"cell_type":"code","execution_count":null,"metadata":{"id":"61f82dbd"},"outputs":[],"source":["# observation_buffer = observation_buffer.clone().detach().to(agent.dev)\n","# action_buffer = action_buffer.clone().detach().to(agent.dev)\n","# advantage_buffer = advantage_buffer.clone().detach().to(agent.dev)\n","# return_buffer = return_buffer.clone().detach().to(agent.dev)\n","# logprobability_buffer = logprobability_buffer.clone().detach().to(agent.dev)\n","# # Calculate new probabilities and ratio\n","# [mu, std], new_values = agent.actorcritic(observation_buffer)\n","# # [mu, std], new_values = [mu.cpu(), std.cpu()], new_values.cpu()\n","\n","\n","# new_action_dist = Normal(mu, std)\n","# new_probs = new_action_dist.log_prob(action_buffer)\n","# ratio = torch.exp(new_probs - logprobability_buffer).mean(1)\n","\n","# # Calculate surrogate loss\n","# unclipped_obj = ratio * advantage_buffer\n","# clipped_obj = torch.clamp(ratio, 1-agent.clip_ratio, 1+agent.clip_ratio) * advantage_buffer\n","# actor_loss = -torch.min(unclipped_obj, clipped_obj).sum()\n","\n","# # Calculate value loss\n","# critic_loss = F.mse_loss(new_values, return_buffer.unsqueeze(1))\n","\n","# # Calculate entropy bonus\n","# entropy_bonus = new_action_dist.entropy().sum()\n","\n","# # Calculate total loss\n","# total_loss = actor_loss + critic_loss - agent.alpha*entropy_bonus\n","# print(total_loss.item(), actor_loss.item(), critic_loss.item(), -agent.alpha*entropy_bonus.item())"],"id":"61f82dbd"},{"cell_type":"code","execution_count":null,"metadata":{"id":"4oqGwzTb0e_h"},"outputs":[],"source":[],"id":"4oqGwzTb0e_h"},{"cell_type":"code","execution_count":null,"metadata":{"id":"c29bbc9a"},"outputs":[],"source":["# plt.plot(new_values.detach().cpu(), label='new_values')"],"id":"c29bbc9a"},{"cell_type":"code","execution_count":null,"metadata":{"id":"15598de9"},"outputs":[],"source":["# plt.plot(new_values.detach().cpu(), label='new_values')\n","# plt.plot(return_buffer.detach().cpu(), label='return_buffer')\n","# plt.legend(loc='best')\n","# plt.show()"],"id":"15598de9"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d6ca6930"},"outputs":[],"source":["# agent.load_weights(20)"],"id":"d6ca6930"},{"cell_type":"code","execution_count":null,"metadata":{"id":"e1a43fe3"},"outputs":[],"source":["a_record, r_record, s_record = agent.history()"],"id":"e1a43fe3"},{"cell_type":"code","execution_count":null,"metadata":{"id":"d4404688"},"outputs":[],"source":["plt.plot(r_record, label='PPO')\n","plt.legend(loc='best')\n","plt.xlabel('time(0.01sec)')\n","plt.ylabel('reward')\n","plt.title(f'Reward for WLTP Cycle')\n","plt.show()"],"id":"d4404688"},{"cell_type":"code","execution_count":null,"metadata":{"id":"7045d799"},"outputs":[],"source":["plt.plot(a_record)"],"id":"7045d799"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b123e343"},"outputs":[],"source":["plt.plot(a_record[:,0], label='Engine Line (PPO)')\n","plt.axhline(y=13.5/20, color='r', linewidth=1, label='Engine on Line (Rule-Based)')\n","plt.axhline(y=2/20, color='r', linewidth=1, label='Engine off Line (Rule-Based)')\n","plt.legend(loc='best')\n","plt.xlabel('time (0.01sec)')\n","plt.ylabel('action')\n","plt.title(f'Instant Engine Line for WLTP Cycle')\n","plt.show()"],"id":"b123e343"},{"cell_type":"code","execution_count":null,"metadata":{"id":"b9b0c857"},"outputs":[],"source":["plt.plot(a_record[:,1], label='OOL (PPO)')\n","plt.axhline(y=0, color='r', linewidth=1, label='OOL (Rule-Based)')\n","plt.legend(loc='best')\n","plt.xlabel('time (0.01sec)')\n","plt.ylabel('action')\n","plt.title(f'Instant OOL for WLTP Cycle')\n","plt.show()"],"id":"b9b0c857"},{"cell_type":"code","execution_count":null,"metadata":{"id":"be95f778"},"outputs":[],"source":["SoC = s_record[:,0]\n","plt.plot(SoC, label='PPO')\n","plt.axhline(y=67, color='r', linewidth=1, label='Initial SoC (67%)')\n","plt.legend(loc='best')\n","plt.xlabel('time (0.01sec)')\n","plt.ylabel('SoC (%)')\n","plt.title(f'Instant SoC for WLTP Cycle')\n","plt.show()"],"id":"be95f778"},{"cell_type":"code","execution_count":null,"metadata":{"id":"1124f366"},"outputs":[],"source":["bsfc = s_record[:,-1]\n","plt.plot(bsfc, label='BSFC')\n","plt.legend(loc='best')\n","plt.xlabel('time (0.01sec)')\n","plt.ylabel('BSFC (g/kW)')\n","plt.title(f'Instant BSFC for WLTP Cycle')\n","plt.show()"],"id":"1124f366"},{"cell_type":"code","execution_count":null,"metadata":{"id":"01d9e020"},"outputs":[],"source":["engine_speed = s_record[:,3]\n","engine_torque = s_record[:,4]\n","plt.scatter(engine_speed, engine_torque, s=0.1, label='PPO')\n","plt.legend(loc='best')\n","plt.xlabel('Engine speed (rpm)')\n","plt.ylabel('Engine torque (Nm)')\n","plt.title('Engine Operating Points for WLTP Cycle')\n","plt.show()"],"id":"01d9e020"}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.9"}},"nbformat":4,"nbformat_minor":5}